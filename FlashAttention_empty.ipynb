{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0c5a0c2-9d32-4303-94d2-ffacd9812ef7",
      "metadata": {
        "id": "a0c5a0c2-9d32-4303-94d2-ffacd9812ef7"
      },
      "source": [
        "```\n",
        " ▄▄▄▄▄▄▄▄▄▄▄  ▄            ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄         ▄                                                     \n",
        "▐░░░░░░░░░░░▌▐░▌          ▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░▌       ▐░▌                                                    \n",
        "▐░█▀▀▀▀▀▀▀▀▀ ▐░▌          ▐░█▀▀▀▀▀▀▀█░▌▐░█▀▀▀▀▀▀▀▀▀ ▐░▌       ▐░▌                                                    \n",
        "▐░▌          ▐░▌          ▐░▌       ▐░▌▐░▌          ▐░▌       ▐░▌                                                    \n",
        "▐░█▄▄▄▄▄▄▄▄▄ ▐░▌          ▐░█▄▄▄▄▄▄▄█░▌▐░█▄▄▄▄▄▄▄▄▄ ▐░█▄▄▄▄▄▄▄█░▌                                                    \n",
        "▐░░░░░░░░░░░▌▐░▌          ▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌                                                    \n",
        "▐░█▀▀▀▀▀▀▀▀▀ ▐░▌          ▐░█▀▀▀▀▀▀▀█░▌ ▀▀▀▀▀▀▀▀▀█░▌▐░█▀▀▀▀▀▀▀█░▌                                                    \n",
        "▐░▌          ▐░▌          ▐░▌       ▐░▌          ▐░▌▐░▌       ▐░▌                                                    \n",
        "▐░▌          ▐░█▄▄▄▄▄▄▄▄▄ ▐░▌       ▐░▌ ▄▄▄▄▄▄▄▄▄█░▌▐░▌       ▐░▌                                                    \n",
        "▐░▌          ▐░░░░░░░░░░░▌▐░▌       ▐░▌▐░░░░░░░░░░░▌▐░▌       ▐░▌                                                    \n",
        " ▀            ▀▀▀▀▀▀▀▀▀▀▀  ▀         ▀  ▀▀▀▀▀▀▀▀▀▀▀  ▀         ▀                                                     \n",
        "                                                                                                                     \n",
        " ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄        ▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄        ▄\n",
        "▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░▌      ▐░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░▌      ▐░▌\n",
        "▐░█▀▀▀▀▀▀▀█░▌ ▀▀▀▀█░█▀▀▀▀  ▀▀▀▀█░█▀▀▀▀ ▐░█▀▀▀▀▀▀▀▀▀ ▐░▌░▌     ▐░▌ ▀▀▀▀█░█▀▀▀▀  ▀▀▀▀█░█▀▀▀▀ ▐░█▀▀▀▀▀▀▀█░▌▐░▌░▌     ▐░▌\n",
        "▐░▌       ▐░▌     ▐░▌          ▐░▌     ▐░▌          ▐░▌▐░▌    ▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌▐░▌    ▐░▌\n",
        "▐░█▄▄▄▄▄▄▄█░▌     ▐░▌          ▐░▌     ▐░█▄▄▄▄▄▄▄▄▄ ▐░▌ ▐░▌   ▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌ ▐░▌   ▐░▌\n",
        "▐░░░░░░░░░░░▌     ▐░▌          ▐░▌     ▐░░░░░░░░░░░▌▐░▌  ▐░▌  ▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌  ▐░▌  ▐░▌\n",
        "▐░█▀▀▀▀▀▀▀█░▌     ▐░▌          ▐░▌     ▐░█▀▀▀▀▀▀▀▀▀ ▐░▌   ▐░▌ ▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌   ▐░▌ ▐░▌\n",
        "▐░▌       ▐░▌     ▐░▌          ▐░▌     ▐░▌          ▐░▌    ▐░▌▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌    ▐░▌▐░▌\n",
        "▐░▌       ▐░▌     ▐░▌          ▐░▌     ▐░█▄▄▄▄▄▄▄▄▄ ▐░▌     ▐░▐░▌     ▐░▌      ▄▄▄▄█░█▄▄▄▄ ▐░█▄▄▄▄▄▄▄█░▌▐░▌     ▐░▐░▌\n",
        "▐░▌       ▐░▌     ▐░▌          ▐░▌     ▐░░░░░░░░░░░▌▐░▌      ▐░░▌     ▐░▌     ▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░▌      ▐░░▌\n",
        " ▀         ▀       ▀            ▀       ▀▀▀▀▀▀▀▀▀▀▀  ▀        ▀▀       ▀       ▀▀▀▀▀▀▀▀▀▀▀  ▀▀▀▀▀▀▀▀▀▀▀  ▀        ▀▀\n",
        "                                                                                       \n",
        " ██▓ ███▄    █    ▄▄▄█████▓ ██▀███   ██▓▄▄▄█████▓ ▒█████   ███▄    █\n",
        "▓██▒ ██ ▀█   █    ▓  ██▒ ▓▒▓██ ▒ ██▒▓██▒▓  ██▒ ▓▒▒██▒  ██▒ ██ ▀█   █\n",
        "▒██▒▓██  ▀█ ██▒   ▒ ▓██░ ▒░▓██ ░▄█ ▒▒██▒▒ ▓██░ ▒░▒██░  ██▒▓██  ▀█ ██▒\n",
        "░██░▓██▒  ▐▌██▒   ░ ▓██▓ ░ ▒██▀▀█▄  ░██░░ ▓██▓ ░ ▒██   ██░▓██▒  ▐▌██▒\n",
        "░██░▒██░   ▓██░     ▒██▒ ░ ░██▓ ▒██▒░██░  ▒██▒ ░ ░ ████▓▒░▒██░   ▓██░\n",
        "░▓  ░ ▒░   ▒ ▒      ▒ ░░   ░ ▒▓ ░▒▓░░▓    ▒ ░░   ░ ▒░▒░▒░ ░ ▒░   ▒ ▒\n",
        " ▒ ░░ ░░   ░ ▒░       ░      ░▒ ░ ▒░ ▒ ░    ░      ░ ▒ ▒░ ░ ░░   ░ ▒░\n",
        " ▒ ░   ░   ░ ░      ░        ░░   ░  ▒ ░  ░      ░ ░ ░ ▒     ░   ░ ░\n",
        " ░           ░                ░      ░               ░ ░           ░                                                                                                                      \n",
        "\n",
        "                                 ,     /~/'   ,--,\n",
        "                               _/`, ,/'/'   /'/~\n",
        "                             .'___|/ /____/'/'   __/|\n",
        "                             /~  __        `\\ /~~, /'\n",
        "                      _,-,__/'  ,       \\   /'/~/ /'\n",
        "                    .~      `   \\_/  / ,     \"~_/'  ,-'~~~~~---,_\n",
        "                    `,               `~    `~~~|   /'    ~~\\__   `~\\_\n",
        "            |~~~/     `~---,__        _,      /'  | /~~\\  _/' ~~\\    `~,\n",
        "            |/\\`\\          /'     _,-~/      /'  .' __ `/'       `~\\    \\\n",
        "   |~~~/       `\\`\\        `-\\/\\/~   /'    .'    |    `| \\/    |    `\\_  |\n",
        "   |/\\`\\         `,`\\              /'      |_  ,' /~\\ /' |' |  `\\     \\~\\|\n",
        "      `\\`\\    _/~~_/~'            /'      /' ~~/     /   `\\ `\\,  | \\   |\n",
        "~/      `\\`\\/~ _/~                ~/~~~~\\/'    `\\__/' \\/\\  `\\_/\\ `\\~~\\ |\n",
        "\\`\\    _/~'    \\               /~~'                `~~~\\`~~~'   `~~'  `'__\n",
        " `\\`\\/~ _/~\\    `\\           /' _/                      `\\        _,-'~~ |\n",
        "   `\\_/~    `\\    `\\       _|--'                          |      `\\     |'\n",
        "              `\\    `\\   /'          _/'                  |       /' /\\|'\n",
        "                /\\/~~\\-/'        _,-'                     |     /' /'  `\n",
        "                |_`\\~~~/`\\     /~                          \\/~~' /'\n",
        "                   |`\\~ \\ `\\   `\\                           `| /'\n",
        "\n",
        "```\n",
        "- by [Marc Lelarge](https://www.di.ens.fr/~lelarge/) - [@marc_lelarge](https://x.com/marc_lelarge)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41532c20-4403-498a-8918-de048cca1f1d",
      "metadata": {
        "id": "41532c20-4403-498a-8918-de048cca1f1d"
      },
      "source": [
        "# Standard Softmax\n",
        "\n",
        "The **softmax operator** transforms a vector of real numbers into a probability distribution. For $x_1,\\dots, x_d \\in \\mathbb{R}$, softmax is defined as:\n",
        "$$\n",
        "\\mathrm{softmax}(x_1, \\ldots, x_d)\n",
        "=\n",
        "\\left(\n",
        "\\frac{e^{x_i}}{\\sum_{j=1}^{d} e^{x_j}}\n",
        "\\right)_{i=1}^{d}\n",
        "$$\n",
        "The output satisfies $\\sum_{i=1}^d \\mathrm{softmax}_i(x_1, \\ldots, x_d) = 1$.\n",
        "\n",
        "## Numerical Stability: Safe Softmax\n",
        "\n",
        "Since $x_i$ may be large, computing $e^{x_i}$ directly can cause numerical overflow. To address this, we use the **safe softmax** trick, which exploits the translation invariance of softmax:\n",
        "$$\n",
        "\\frac{e^{x_i}}{\\sum_{j=1}^{d} e^{x_j}}\n",
        "=\n",
        "\\frac{e^{x_i - m}}{\\sum_{j=1}^{d} e^{x_j - m}}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "m = \\max_{j=1,\\ldots,d} x_j\n",
        "$$\n",
        "This ensures $x_i - m \\leq 0$ for all $i$, making the exponential computation numerically stable since $e^{x_i - m} \\in (0, 1]$.\n",
        "\n",
        "## Matrix Extension\n",
        "\n",
        "For matrices $x \\in \\mathbb{R}^{d_1 \\times d_2}$, we extend softmax by applying it **column-wise**. Each $x_i \\in \\mathbb{R}^{d_1}$ denotes the $i$-th column of $x$ (for $i=1,\\ldots,d_2$), and we compute:\n",
        "$$\n",
        "\\mathrm{softmax}(x)_{ij} = \\frac{e^{x_{ij} - m_i}}{\\sum_{k=1}^{d_2} e^{x_{ik} - m_i}}\n",
        "$$\n",
        "where $m_i = \\max_{k=1,\\ldots,d_2} x_{ik}$ is computed along the $i$-th row. For each row index $i$, the outputs form a probability distribution over the columns:\n",
        "$$\n",
        "\\sum_{j=1}^{d_2} \\mathrm{softmax}(x)_{ij} = 1\n",
        "$$\n",
        "To simplify notation, we will use all operations component wise so that we can still write\n",
        "$$\n",
        "\\mathrm{softmax}(x_1, \\ldots, x_{d_2})\n",
        "=\n",
        "\\left(\n",
        "\\frac{e^{x_i- m}}{\\sum_{j=1}^{d_2} e^{x_j- m}}\n",
        "\\right)_{i=1}^{d_2},\n",
        "$$\n",
        "where $e^{x_i}\\in \\mathbb{R}^{d_1}$ is applied component-wise like the maximum $m =\\max_{j=1,\\ldots,d_2} x_j \\in \\mathbb{R}^{d_1}$.\n",
        "\n",
        "The naive implementation of the safe softmax requires 3 passes on the data $x$: the first pass compute the max $m$, the second pass compute the denominator $\\ell = \\sum_{j=1}^{d_2} e^{x_j- m}$ and the last pass compute the softmax.\n",
        "\n",
        "## Algorithm: (safe) softmax\n",
        "\n",
        "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R^{d_1}}$  \n",
        "**Output:** $s_1, \\ldots, s_{d_2}$ (softmax values)\n",
        "\n",
        "**Step 1:** Compute the maximum\n",
        "$$\n",
        "\\begin{align}\n",
        "m_0 &= -\\infty \\\\\n",
        "m_i &\\leftarrow \\max(m_{i-1}, x_i) \\quad \\text{for } i = 1, \\ldots, d_2\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Step 2:** Compute the normalizing constant\n",
        "$$\n",
        "\\begin{align}\n",
        "\\ell_0 &= 0 \\\\\n",
        "\\ell_i &\\leftarrow \\ell_{i-1} + e^{x_i - m_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Step 3:** Compute softmax values\n",
        "$$\n",
        "s_i \\leftarrow \\frac{e^{x_i - m_{d_2}}}{\\ell_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
        "$$\n",
        "Below is the corresponding code in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d42e39-80c9-4539-8aa1-d8892b456e77",
      "metadata": {
        "id": "b5d42e39-80c9-4539-8aa1-d8892b456e77"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import urlretrieve  # For downloading files from URLs\n",
        "from pathlib import Path\n",
        "from torchvision import io\n",
        "import torchvision.transforms.functional as tvf\n",
        "\n",
        "import triton\n",
        "import triton.language as tl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7026e6ee-8d35-4d96-aaef-cde16aa4e47a",
      "metadata": {
        "id": "7026e6ee-8d35-4d96-aaef-cde16aa4e47a"
      },
      "outputs": [],
      "source": [
        "def softmax(x, dim=-1):\n",
        "    rescaled_input = x - torch.max(x, dim=dim, keepdim=True)[0]\n",
        "    exponentiated_rescaled_input = torch.exp(rescaled_input)\n",
        "    return exponentiated_rescaled_input / torch.sum(\n",
        "        exponentiated_rescaled_input, dim=dim, keepdim=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb896ddb-17d8-49a1-8c4f-1fdfec1948ea",
      "metadata": {
        "id": "bb896ddb-17d8-49a1-8c4f-1fdfec1948ea"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8,16).cuda()\n",
        "y = softmax(x)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ae9524-1e77-4f94-8aa1-2141dca1941f",
      "metadata": {
        "id": "22ae9524-1e77-4f94-8aa1-2141dca1941f"
      },
      "outputs": [],
      "source": [
        "ATOL = RTOL = 1e-5\n",
        "torch.testing.assert_close(y.sum(-1), torch.ones(x.shape[0]).cuda(), atol=ATOL, rtol=RTOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38ec22f-4d21-45c8-a7cb-8035456934c5",
      "metadata": {
        "id": "c38ec22f-4d21-45c8-a7cb-8035456934c5"
      },
      "source": [
        "# Online Softmax\n",
        "\n",
        "Naive implementation requires 3 global memory accesses, we would like to decrease this number of accesses. In order to compute $\\ell_i = \\sum_{j\\leq i} e^{x_j-m_{d_2}}$, we need to have made one pass in order to compute $m_{d_2}$.\n",
        "Instead, define $\\ell'_i = \\sum_{j\\leq i} e^{x_j-m_i}$ so that we get:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\ell'_i &= \\left( \\sum_{j\\leq i-1} e^{x_j-m_{i-1}} \\right) e^{m_{i-1}-m_i} + e^{x_i-m_i}\\\\\n",
        "&= \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i-m_i},\n",
        "\\end{align}\n",
        "$$\n",
        "and of course $\\ell'_{d_2} = \\ell_{d_2}$ the desired normalizing constant. Since computing $\\ell'_i$ only requires the quantities $m_{i-1}, m_i$ and $x_i$, it can be computed during the first pass.\n",
        "\n",
        "## Algorithm: online softmax\n",
        "\n",
        "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R^{d_1}}$  \n",
        "**Output:** $s_1, \\ldots, s_{d_2}$ (softmax values)\n",
        "\n",
        "**Step 1:** Compute the maximum and the normalizing constant\n",
        "$$\n",
        "\\begin{align}\n",
        "m_0 &= -\\infty \\\\\n",
        "\\ell'_0 &= 0 \\\\\n",
        "\\text{for }& i = 1, \\ldots, d_2\\\\\n",
        "& m_i \\leftarrow \\max(m_{i-1}, x_i) \\\\\n",
        "& \\ell'_i \\leftarrow \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i - m_i}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Step 3:** Compute softmax values\n",
        "$$\n",
        "s_i \\leftarrow \\frac{e^{x_i - m_{d_2}}}{\\ell'_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
        "$$\n",
        "Below is the corresponding code in PyTorch (parameter `B` allows to make computations in \"blocks\" (taking the maximum and summing)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9609f30-0773-45e8-bc51-e9088ee1e3e1",
      "metadata": {
        "id": "c9609f30-0773-45e8-bc51-e9088ee1e3e1"
      },
      "outputs": [],
      "source": [
        "def online_softmax(x, B=1):\n",
        "    *bs, d = x.shape\n",
        "    device = x.device\n",
        "    Td = math.ceil(d / B)\n",
        "    m_prev = torch.full((*bs, 1), float(\"-inf\"), device=device)  # current max\n",
        "    l_prev = torch.zeros((*bs, 1), device=device)  # current sum of exps\n",
        "\n",
        "    for i in range(Td):\n",
        "        start = i * B\n",
        "        end = min((i + 1) * B, d)\n",
        "        x_block = x[..., start:end]\n",
        "\n",
        "        block_max = x_block.max(dim=-1, keepdim=True).values\n",
        "        m_curr = torch.maximum(m_prev, block_max)\n",
        "\n",
        "        l_prev = l_prev * torch.exp(m_prev - m_curr) + torch.exp(x_block - m_curr).sum(\n",
        "            dim=-1, keepdim=True\n",
        "        )\n",
        "        m_prev = m_curr\n",
        "\n",
        "    softmax_output = torch.empty_like(x)\n",
        "    for i in range(Td):\n",
        "        start = i * B\n",
        "        end = min((i + 1) * B, d)\n",
        "        softmax_output[..., start:end] = torch.exp(x[..., start:end] - m_prev)\n",
        "\n",
        "    return softmax_output / l_prev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "248df682-95c0-426e-b1de-14fc854b4e17",
      "metadata": {
        "id": "248df682-95c0-426e-b1de-14fc854b4e17"
      },
      "outputs": [],
      "source": [
        "torch.testing.assert_close(y, online_softmax(x), atol=ATOL, rtol=RTOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16321bf7-2603-4283-b584-fe36e13f9a95",
      "metadata": {
        "id": "16321bf7-2603-4283-b584-fe36e13f9a95"
      },
      "source": [
        "# Triton implementation of online softmax\n",
        "\n",
        "Before using [Triton](https://triton-lang.org/main/index.html), we give a PyTorch implementaion to highlight the main concepts to code in Triton. For more details on GPU programming, we recommend to have a look at the notebook: [Programming on GPUs](https://github.com/dataflowr/notebooks/blob/master/ModuleGPU/GPU_programming_basics.ipynb)\n",
        "\n",
        "## Triton Programming Model\n",
        "\n",
        "Triton uses an **SPMD (Single Program, Multiple Data)** approach where the same kernel code runs in parallel across multiple \"program instances,\" each processing a different block of data.\n",
        "\n",
        "### Key concepts illustrated in the (PyTorch) code below:\n",
        "\n",
        "1. **Program ID (`fake_pid`)**: Each kernel instance has a unique program ID that determines which data block it processes. In the code, `fake_pid = (i, j)` identifies the batch index `i` and tile index `j`.\n",
        "\n",
        "2. **Block/Tile-based processing**: The input matrix is divided into blocks (e.g., `block_1 × d_2`). Each program instance loads and processes one tile:\n",
        "```python\n",
        "   x_block = x_ptr[fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, :]\n",
        "```\n",
        "Inside each program, each block is subdivided into smaller tiles of size `block_1 x block_2` to make the computation of the online softmax for the corresponding initial block:\n",
        "```python\n",
        "   for i in range(Num_blocks):\n",
        "        start = i * block_2\n",
        "        end = min((i + 1) * block_2, d_2)\n",
        "        x_load = x_block[:, start:end]\n",
        "       ...\n",
        "```\n",
        "\n",
        "3. **Parallel execution**: Multiple kernel instances run concurrently, each with a different `pid`. The shuffling in `online_softmax_fake_triton` simulates this parallelism:\n",
        "```python\n",
        "   all_blocks = [(i, j) for i in range(bs) for j in range(Num_tiles)]\n",
        "   random.shuffle(all_blocks)  # Simulates arbitrary execution order\n",
        "```\n",
        "\n",
        "4. **Memory hierarchy**: Blocks are sized to fit in GPU SRAM (Triton's fast shared memory), though this Python simulation doesn't explicitly model the memory transfer.\n",
        "\n",
        "The programmer writes a single kernel function that operates on one block, and Triton automatically launches many parallel instances across all blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fac2c0-d1ba-43d5-bbcd-e185b8c0a69a",
      "metadata": {
        "id": "b3fac2c0-d1ba-43d5-bbcd-e185b8c0a69a"
      },
      "outputs": [],
      "source": [
        "def online_softmax_kernel(x_ptr, block_1, block_2, d_2, fake_pid):\n",
        "    device = x_ptr.device\n",
        "    x_block = x_ptr[\n",
        "            fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, :\n",
        "        ]\n",
        "\n",
        "    m_prev = torch.full((block_1, 1), float(\"-inf\"), device=device)  # current max\n",
        "    l_prev = torch.zeros((block_1, 1), device=device)  # current sum of exps\n",
        "    Num_blocks = math.ceil(d_2 / block_2)\n",
        "\n",
        "    for i in range(Num_blocks):\n",
        "        start = i * block_2\n",
        "        end = min((i + 1) * block_2, d_2)\n",
        "        x_load = x_block[:, start:end]\n",
        "\n",
        "        block_max = x_load.max(dim=-1, keepdim=True).values\n",
        "        m_curr = torch.maximum(m_prev, block_max)\n",
        "\n",
        "        l_prev = l_prev * torch.exp(m_prev - m_curr) + torch.exp(x_load - m_curr).sum(\n",
        "            dim=-1, keepdim=True\n",
        "        )\n",
        "        m_prev = m_curr\n",
        "\n",
        "    softmax_output = torch.empty_like(\n",
        "        x_block\n",
        "    )\n",
        "    for i in range(Num_blocks):\n",
        "        start = i * block_2\n",
        "        end = min((i + 1) * block_2, d_2)\n",
        "        x_load = x_block[:, start:end]\n",
        "        softmax_output[..., start:end] = torch.exp(x_load - m_prev)\n",
        "\n",
        "    return softmax_output / l_prev\n",
        "\n",
        "\n",
        "def online_softmax_fake_triton(x, B=16):\n",
        "    assert x.shape[1] % B == 0, \"d1 must be a multiple of B for fake triton kernel\"\n",
        "    bs, d1, d2 = x.shape\n",
        "    Num_tiles = math.ceil(d1 / B)\n",
        "    softmax_output = torch.empty_like(x)\n",
        "\n",
        "    # Create list of all (batch, tile) pairs and shuffle to show parallelism\n",
        "    all_blocks = [(i, j) for i in range(bs) for j in range(Num_tiles)]\n",
        "    random.shuffle(all_blocks) # Simulates arbitrary execution order\n",
        "\n",
        "    for i, j in all_blocks:\n",
        "        fake_pid = (i, j)\n",
        "        softmax_output[i, j * B : min((j + 1) * B, d1), :] = online_softmax_kernel(\n",
        "            x, B, B, d2, fake_pid\n",
        "        )\n",
        "    return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a8b561c-09c2-4677-a403-e4ddf2163961",
      "metadata": {
        "id": "9a8b561c-09c2-4677-a403-e4ddf2163961"
      },
      "outputs": [],
      "source": [
        "B = 8\n",
        "x = torch.randn(2,8,16).cuda()\n",
        "y = softmax(x)\n",
        "torch.testing.assert_close(y, online_softmax_fake_triton(x, B=B), atol=ATOL, rtol=RTOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4f09b2-f8b1-4137-aed8-d77c8cff9f39",
      "metadata": {
        "id": "0f4f09b2-f8b1-4137-aed8-d77c8cff9f39"
      },
      "source": [
        "### Offset, Stride and Padding\n",
        "\n",
        "Why do we need?\n",
        "```python\n",
        "assert x.shape[1] % B == 0, \"d1 must be a multiple of B for fake triton kernel\"\n",
        "```\n",
        "Example that breaks\n",
        "```python\n",
        "x = torch.randn(1, 10, 8).cuda()  # d1=10, d2=8\n",
        "B = 3\n",
        "\n",
        "# Num_tiles = ceil(10/3) = 4\n",
        "# Tiles: [0:3], [3:6], [6:9], [9:12] -> but 9:12 is only [9:10] = 1 element!\n",
        "```\n",
        "\n",
        "For the last tile (j=3):\n",
        "- `x_block` shape: (1, 8)  ← only 1 row!\n",
        "- `m_prev` shape: (3, 1)   ← expects 3 rows!\n",
        "- `l_prev` shape: (3, 1)   ← expects 3 rows!\n",
        "\n",
        "When computing:\n",
        "```python\n",
        "x_load = x_block[:, 0:3]  # shape (1, 3)\n",
        "block_max = x_load.max(dim=-1, keepdim=True).values  # shape (1, 1)\n",
        "m_curr = torch.maximum(m_prev, block_max)  # broadcasts to (3, 1)\n",
        "```\n",
        "Then this breaks:\n",
        "```python\n",
        "torch.exp(x_load - m_curr)\n",
        "```\n",
        "shapes: (1, 3) - (3, 1) = broadcasts to (3, 3) ← WRONG!\n",
        "Should be (1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d1b032-702d-408d-adea-59ccad5d5ca7",
      "metadata": {
        "id": "79d1b032-702d-408d-adea-59ccad5d5ca7"
      },
      "source": [
        "## Triton Online Softmax Implementation\n",
        "\n",
        "This kernel computes softmax along dimension `d2` (columns) using the online algorithm, processing the matrix in `BLOCK_1 × BLOCK_2` tiles.\n",
        "\n",
        "### Kernel Launch and Parallelization\n",
        "```python\n",
        "grid = (batch_size, triton.cdiv(d1, BLOCK_1))\n",
        "```\n",
        "\n",
        "The grid defines how many parallel program instances run:\n",
        "- **Dimension 0**: One instance per batch element\n",
        "- **Dimension 1**: One instance per `BLOCK_1` rows\n",
        "\n",
        "Each program instance gets its ID via:\n",
        "```python\n",
        "pid_batch = tl.program_id(0)  # Which batch element\n",
        "pid_row = tl.program_id(1)    # Which block of rows (out of d1/BLOCK_1 blocks)\n",
        "```\n",
        "\n",
        "### Memory Layout: Block Pointers and Strides\n",
        "```python\n",
        "x_block = tl.make_block_ptr(\n",
        "    x_ptr + pid_batch * stride_xbatch,\n",
        "    shape=(d1, d2),\n",
        "    strides=(stride_xrow, stride_xcol),\n",
        "    offsets=(pid_row * BLOCK_1, 0),\n",
        "    block_shape=(BLOCK_1, BLOCK_2),\n",
        "    order=(1, 0),\n",
        ")\n",
        "```\n",
        "\n",
        "**Block pointers** provide a structured way to access multi-dimensional data:\n",
        "- `x_ptr + pid_batch * stride_xbatch`: Start at the correct batch element\n",
        "- `shape=(d1, d2)`: the full logical size of the parent tensor region\n",
        "- `offsets=(pid_row * BLOCK_1, 0)`: Start at row `pid_row * BLOCK_1`, column 0\n",
        "- `block_shape=(BLOCK_1, BLOCK_2)`: Each load reads a `BLOCK_1 × BLOCK_2` tile\n",
        "- `order=(1, 0)`: Memory layout (column-major style indexing)\n",
        "- **Strides** handle arbitrary tensor layouts (contiguous, transposed, etc.)\n",
        "\n",
        "### Algorithm: Two-Pass Online Softmax\n",
        "\n",
        "**Pass 1: Compute global statistics**\n",
        "```python\n",
        "for _ in range(Num_blocks):\n",
        "    x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "    \n",
        "    block_max = tl.max(x, axis=1)  # Max across columns in this block\n",
        "    m_curr = tl.maximum(m_prev, block_max)  # Update global max\n",
        "    \n",
        "    # Rescale previous sum and add current block's contribution\n",
        "    exp_x_block = tl.exp(x - m_curr[:, None])\n",
        "    l_prev = l_prev * tl.exp(m_prev - m_curr) + tl.sum(exp_x_block, axis=1)\n",
        "    m_prev = m_curr\n",
        "    \n",
        "    x_block = x_block.advance((0, BLOCK_2))  # Move to next column block\n",
        "```\n",
        "\n",
        "This implements step 1 from the algorithm:\n",
        "- `m_prev`: Running maximum across all columns seen so far (shape: `BLOCK_1`)\n",
        "- `l_prev`: Running sum of exponentials, rescaled when max updates (shape: `BLOCK_1`)\n",
        "- `x_block.advance((0, BLOCK_2))`: Slides the block pointer right by `BLOCK_2` columns\n",
        "\n",
        "**Pass 2: Compute and store softmax**\n",
        "```python\n",
        "for _ in range(Num_blocks):\n",
        "    x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "    \n",
        "    tl.store(\n",
        "        softmax_block,\n",
        "        tl.exp(x - m_prev[:, None]) / l_prev[:, None],\n",
        "        boundary_check=(0, 1),\n",
        "    )\n",
        "    \n",
        "    x_block = x_block.advance((0, BLOCK_2))\n",
        "    softmax_block = softmax_block.advance((0, BLOCK_2))\n",
        "```\n",
        "\n",
        "Now that we have `m_prev` and `l_prev`, we compute softmax for each block and write results.\n",
        "\n",
        "### Key Triton Features\n",
        "\n",
        "1. **`tl.constexpr`**: Compile-time constants that enable optimizations\n",
        "```python\n",
        "   d1: tl.constexpr, d2: tl.constexpr, BLOCK_1: tl.constexpr, BLOCK_2: tl.constexpr\n",
        "```\n",
        "\n",
        "2. **Broadcasting with `[:, None]`**: Expands `m_prev` from shape `(BLOCK_1,)` to `(BLOCK_1, 1)` for broadcasting across columns\n",
        "```python\n",
        "   tl.exp(x - m_prev[:, None])  # Subtract row-wise max from all columns\n",
        "```\n",
        "\n",
        "3. **Boundary checking**: Handles edge cases when dimensions aren't perfect multiples\n",
        "```python\n",
        "   tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "```\n",
        "\n",
        "4. **`tl.static_assert`**: Compile-time checks ensuring valid block sizes\n",
        "```python\n",
        "   tl.static_assert(d2 % BLOCK_2 == 0, \"d2 must be divisible by BLOCK_2\")\n",
        "```\n",
        "This is due to the padding above which should be $-\\infty$ but not possible in triton.\n",
        "\n",
        "5. **`tl.device_print`** might be usefull for debugging.\n",
        "\n",
        "### Why This Design?\n",
        "\n",
        "- **Memory efficiency**: Only loads `BLOCK_1 × BLOCK_2` elements at a time (fits in SRAM)\n",
        "- **Two passes required**: Must see all columns before computing final softmax values\n",
        "- **Parallel across rows**: Different program instances handle different row blocks independently\n",
        "- **Streaming across columns**: Processes columns sequentially within each program instance to stay within memory limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e1fe7e-6499-4a30-9b02-30bcdd9065f7",
      "metadata": {
        "id": "f5e1fe7e-6499-4a30-9b02-30bcdd9065f7"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def online_softmax_triton_kernel(\n",
        "    x_ptr,\n",
        "    softmax_ptr,\n",
        "    stride_xbatch,\n",
        "    stride_xrow,\n",
        "    stride_xcol,\n",
        "    stride_sbatch,\n",
        "    stride_srow,\n",
        "    stride_scol,\n",
        "    d1: tl.constexpr,\n",
        "    d2: tl.constexpr,\n",
        "    BLOCK_1: tl.constexpr,\n",
        "    BLOCK_2: tl.constexpr,\n",
        "):\n",
        "\n",
        "    tl.static_assert(d2 % BLOCK_2 == 0, \"d2 must be divisible by BLOCK_2\")\n",
        "    tl.static_assert(d1 % BLOCK_1 == 0, \"d1 must be divisible by BLOCK_1\")\n",
        "\n",
        "    # Each program handles one block of rows (BLOCK_1 rows)\n",
        "    pid_batch = tl.program_id(0)\n",
        "    pid_row = tl.program_id(1)\n",
        "\n",
        "    x_block = tl.make_block_ptr(\n",
        "        x_ptr + pid_batch * stride_xbatch,\n",
        "        shape=(d1, d2),\n",
        "        strides=(stride_xrow, stride_xcol),\n",
        "        offsets=(pid_row * BLOCK_1, 0),\n",
        "        block_shape=(BLOCK_1, BLOCK_2),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "\n",
        "    # Number of blocks in the column dimension\n",
        "    Num_blocks = tl.cdiv(d2, BLOCK_2)\n",
        "\n",
        "    # Initialize m_prev and l_prev for this block of rows\n",
        "    m_prev = tl.full((BLOCK_1,), float(\"-inf\"), dtype=tl.float32)\n",
        "    l_prev = tl.zeros((BLOCK_1,), dtype=tl.float32)\n",
        "\n",
        "    # First pass: compute global max and sum\n",
        "    for _ in range(Num_blocks):\n",
        "        x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "\n",
        "        # Compute block max\n",
        "        block_max = tl.max(x, axis=1)\n",
        "        m_curr = tl.maximum(m_prev, block_max)\n",
        "\n",
        "        # Update running sum with rescaling\n",
        "        exp_x_block = tl.exp(x - m_curr[:, None])\n",
        "        l_prev = l_prev * tl.exp(m_prev - m_curr) + tl.sum(exp_x_block, axis=1)\n",
        "        m_prev = m_curr\n",
        "\n",
        "        x_block = x_block.advance((0, BLOCK_2))\n",
        "\n",
        "    # Second pass: compute and store softmax output\n",
        "    softmax_block = tl.make_block_ptr(\n",
        "        softmax_ptr + pid_batch * stride_sbatch,\n",
        "        shape=(d1, d2),\n",
        "        strides=(stride_srow, stride_scol),\n",
        "        offsets=(pid_row * BLOCK_1, 0),\n",
        "        block_shape=(BLOCK_1, BLOCK_2),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    x_block = tl.make_block_ptr(\n",
        "        x_ptr + pid_batch * stride_xbatch,\n",
        "        shape=(d1, d2),\n",
        "        strides=(stride_xrow, stride_xcol),\n",
        "        offsets=(pid_row * BLOCK_1, 0),\n",
        "        block_shape=(BLOCK_1, BLOCK_2),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    # tl.device_print(\"m_prev:\", m_prev)\n",
        "    # tl.device_print(\"l_prev:\", l_prev)\n",
        "\n",
        "    for _ in range(Num_blocks):\n",
        "        x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
        "        # Compute softmax for this block\n",
        "        tl.store(\n",
        "            softmax_block,\n",
        "            tl.exp(x - m_prev[:, None]) / l_prev[:, None],\n",
        "            boundary_check=(0, 1),\n",
        "        )\n",
        "\n",
        "        x_block = x_block.advance((0, BLOCK_2))\n",
        "        softmax_block = softmax_block.advance((0, BLOCK_2))\n",
        "\n",
        "\n",
        "def online_softmax_triton(x, BLOCK_1=16, BLOCK_2=16):\n",
        "    \"\"\"\n",
        "    Compute softmax using Triton kernel with online algorithm.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor of shape (batch_size, d1, d2)\n",
        "        BLOCK_1: Block size for dimension d1 (rows)\n",
        "        BLOCK_2: Block size for dimension d2 (columns, softmax dimension)\n",
        "    \"\"\"\n",
        "    batch_size, d1, d2 = x.shape\n",
        "    softmax_output = torch.empty_like(x)\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    grid = (batch_size, triton.cdiv(d1, BLOCK_1))\n",
        "\n",
        "    # Launch kernel\n",
        "    online_softmax_triton_kernel[grid](\n",
        "        x,\n",
        "        softmax_output,\n",
        "        x.stride(0),\n",
        "        x.stride(1),\n",
        "        x.stride(2),\n",
        "        softmax_output.stride(0),\n",
        "        softmax_output.stride(1),\n",
        "        softmax_output.stride(2),\n",
        "        d1,\n",
        "        d2,\n",
        "        BLOCK_1=BLOCK_1,\n",
        "        BLOCK_2=BLOCK_2,\n",
        "    )\n",
        "\n",
        "    return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6f7e1d-27cd-44e0-93f3-b287c66dea36",
      "metadata": {
        "id": "ec6f7e1d-27cd-44e0-93f3-b287c66dea36"
      },
      "outputs": [],
      "source": [
        "B = 8\n",
        "x = torch.randn(2,16,32).cuda()\n",
        "y = softmax(x)\n",
        "torch.testing.assert_close(y, online_softmax_triton(x, BLOCK_1=B, BLOCK_2=2*B), atol=ATOL, rtol=RTOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "332b8b7b-b3f6-4416-ad0f-c2ccac7a4c94",
      "metadata": {
        "id": "332b8b7b-b3f6-4416-ad0f-c2ccac7a4c94"
      },
      "source": [
        "## GPU Benchmarking with CUDA Events\n",
        "\n",
        "### Why `torch.cuda.synchronize()` is Critical\n",
        "\n",
        "**GPU operations are asynchronous:**\n",
        "- When you call a CUDA operation (e.g., `x @ y`), Python immediately returns without waiting\n",
        "- The GPU kernel is queued and executes later\n",
        "- The CPU continues running Python code while GPU works in the background\n",
        "\n",
        "**Example without synchronization (WRONG):**\n",
        "```python\n",
        "start = time.time()\n",
        "result = heavy_gpu_computation()  # Returns immediately!\n",
        "end = time.time()\n",
        "print(end - start)  # ❌ Measures ~0ms (only kernel launch overhead)\n",
        "```\n",
        "\n",
        "### The Role of Each `synchronize()` Call\n",
        "\n",
        "**First `torch.cuda.synchronize()` (before `start.record()`):**\n",
        "```python\n",
        "torch.cuda.synchronize()  # Ensure clean slate\n",
        "start.record()\n",
        "```\n",
        "- **Purpose:** Wait for all previous GPU operations to complete\n",
        "- **Why needed:** Without this, previous iterations' work might still be running\n",
        "- **Effect:** Ensures we're measuring only `fn()`, not leftover work from earlier\n",
        "\n",
        "**Second `torch.cuda.synchronize()` (after `end.record()`):**\n",
        "```python\n",
        "end.record()\n",
        "torch.cuda.synchronize()  # Wait for fn() to finish\n",
        "times.append(start.elapsed_time(end))\n",
        "```\n",
        "- **Purpose:** Wait for `fn()` to actually complete\n",
        "- **Why needed:** Without this, `elapsed_time()` might be called before the work is done\n",
        "- **Effect:** Ensures timing measurement is accurate\n",
        "\n",
        "### CUDA Events for Accurate Timing\n",
        "```python\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "```\n",
        "\n",
        "**CUDA Events** are markers placed in the GPU command stream:\n",
        "- `start.record()`: Places a timestamp marker *on the GPU*\n",
        "- `end.record()`: Places another timestamp marker *on the GPU*\n",
        "- `start.elapsed_time(end)`: Computes GPU-side time between markers\n",
        "\n",
        "**Why use Events instead of `time.time()`?**\n",
        "- Events measure actual GPU execution time (excluding CPU-GPU communication)\n",
        "- `time.time()` would include Python overhead, kernel launch latency, etc.\n",
        "- Events are synchronized with the GPU command stream\n",
        "\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Always synchronize** before and after the code you're timing\n",
        "2. **Use CUDA Events** for GPU timing (not `time.time()`)\n",
        "3. **Warmup iterations**: Run `fn()` a few times before timing to avoid one-time costs (JIT compilation, memory allocation)\n",
        "4. **Multiple iterations**: Run many times and compute statistics (median, percentiles) to account for variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae800459-f1e4-4ea6-8779-a6fee7560524",
      "metadata": {
        "id": "ae800459-f1e4-4ea6-8779-a6fee7560524"
      },
      "outputs": [],
      "source": [
        "def time_loop(fn, iters, warmup=5):\n",
        "    # Warmup phase\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Timing phase\n",
        "    times = []\n",
        "    for _ in range(iters):\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        torch.cuda.synchronize()  # Wait for previous work to finish\n",
        "        start.record()            # Record start event\n",
        "        fn()                      # Execute the function\n",
        "        end.record()              # Record end event\n",
        "        torch.cuda.synchronize()  # Wait for fn() to finish\n",
        "\n",
        "        times.append(start.elapsed_time(end))\n",
        "    return times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecf397de-fd4d-474d-ba83-3a8a9f21afd2",
      "metadata": {
        "id": "ecf397de-fd4d-474d-ba83-3a8a9f21afd2"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "dtype = torch.float32\n",
        "batch_size = 8\n",
        "d1 = 2048\n",
        "d2 = 8192\n",
        "B = 32\n",
        "x = torch.randn(batch_size, d1, d2, device=device, dtype=dtype)\n",
        "\n",
        "times_triton = time_loop(lambda: online_softmax_triton(x, BLOCK_1=B, BLOCK_2=B), 10)\n",
        "\n",
        "times_naive = time_loop(lambda: softmax(x), 10)\n",
        "\n",
        "times_pytorch = time_loop(lambda: F.softmax(x, dim=-1), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4074cae2-fa4f-435f-ac7f-d90595408ece",
      "metadata": {
        "id": "4074cae2-fa4f-435f-ac7f-d90595408ece"
      },
      "outputs": [],
      "source": [
        "print(f\"NAIVE bench ms_mean: {np.mean(times_naive)} and ms_std: {np.std(times_naive)}\")\n",
        "print(f\"TRITON bench ms_mean: {np.mean(times_triton)} and ms_std: {np.std(times_triton)}\")\n",
        "print(f\"PYTORCH bench ms_mean: {np.mean(times_pytorch)} and ms_std: {np.std(times_pytorch)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af4555f3-855c-4b12-8a74-022e536f7561",
      "metadata": {
        "id": "af4555f3-855c-4b12-8a74-022e536f7561"
      },
      "source": [
        "Our current Triton kernel is slower than PyTorch implementation but it's predictable, loyal ;-)\n",
        "<video width=\"320\" height=\"240\"\n",
        "       src=\"https://www.di.ens.fr/~lelarge/Macron_Davos.mp4\"  \n",
        "       controls>\n",
        "</video>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c978bb-af22-4f56-8501-92986e25178e",
      "metadata": {
        "id": "13c978bb-af22-4f56-8501-92986e25178e"
      },
      "source": [
        "# Exercise: RGB to Grayscale Conversion with Triton Block Pointers\n",
        "\n",
        "In this exercise, you will learn how to refactor a Triton kernel from using manual pointer arithmetic to using `tl.make_block_ptr` for cleaner and safer memory access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47d8241-eb3b-461f-ab89-c18ca95716f9",
      "metadata": {
        "id": "a47d8241-eb3b-461f-ab89-c18ca95716f9"
      },
      "outputs": [],
      "source": [
        "# Define image URL and download if not already present\n",
        "url = \"https://images.unsplash.com/photo-1543466835-00a7907e9de1?w=800\"\n",
        "path_img = Path('puppy.jpg')\n",
        "\n",
        "if not path_img.exists():\n",
        "    urlretrieve(url, path_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154b174e-e9d7-43c3-a29e-285fdfb41000",
      "metadata": {
        "id": "154b174e-e9d7-43c3-a29e-285fdfb41000"
      },
      "outputs": [],
      "source": [
        "# Read the image using torchvision\n",
        "img = io.read_image('puppy.jpg')\n",
        "print(f\"Image shape (channels, height, width): {img.shape}\")\n",
        "img[:2,:3,:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035afd5d-3971-4af6-93c2-752a7daee57e",
      "metadata": {
        "id": "035afd5d-3971-4af6-93c2-752a7daee57e"
      },
      "outputs": [],
      "source": [
        "def show_img(x, figsize=(4,3), **kwargs):\n",
        "    \"\"\"\n",
        "    Display an image using matplotlib\n",
        "\n",
        "    Args:\n",
        "        x: Image tensor\n",
        "        figsize: Figure size in inches (width, height)\n",
        "        **kwargs: Additional arguments passed to plt.imshow()\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis('off')\n",
        "    # Convert from CHW (channels, height, width) to HWC format if needed\n",
        "    if len(x.shape) == 3:\n",
        "        x = x.permute(1, 2, 0)\n",
        "    plt.imshow(x.cpu(), **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b82f1b2-2a84-4d59-a3c7-585e31ba6bcf",
      "metadata": {
        "id": "0b82f1b2-2a84-4d59-a3c7-585e31ba6bcf"
      },
      "outputs": [],
      "source": [
        "# Resize image to a smaller size for faster processing\n",
        "img = tvf.resize(img, 150, antialias=True)\n",
        "ch, h, w = img.shape  # Get channels, height, and width\n",
        "ch,h,w,h*w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edfd20dd-e295-426a-a6ca-e180d2655bc3",
      "metadata": {
        "id": "edfd20dd-e295-426a-a6ca-e180d2655bc3"
      },
      "outputs": [],
      "source": [
        "show_img(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13e9da83-b0c2-4e3f-921f-9ce32ce32fa3",
      "metadata": {
        "id": "13e9da83-b0c2-4e3f-921f-9ce32ce32fa3"
      },
      "source": [
        "## Objective\n",
        "Convert a color image to grayscale using a Triton kernel.\n",
        "\n",
        "## Implementation Steps\n",
        "\n",
        "1. **Load Image Data** – Use an image (e.g., a puppy image) as input\n",
        "\n",
        "2. **Calculate 2D Offsets** – Compute row and column offsets using broadcasting to create a grid\n",
        "\n",
        "3. **Create Masks** – Handle image boundaries to avoid out-of-bounds access\n",
        "\n",
        "4. **Load Color Channels** – Load R, G, and B values using offsets\n",
        "\n",
        "5. **Compute Grayscale Values** – Apply formula: `grayscale = 0.2989*R + 0.5870*G + 0.1140*B`\n",
        "\n",
        "6. **Store Grayscale Data** – Write the grayscale values back to the output tensor\n",
        "\n",
        "Below you have the code from [GPU MODE Lecture 14: Practitioners Guide to Triton](https://christianjmills.com/posts/cuda-mode-notes/lecture-014/#example-2-grayscaling-an-image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9234415c-e05f-428b-b3f8-1d65e83da8c4",
      "metadata": {
        "id": "9234415c-e05f-428b-b3f8-1d65e83da8c4"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def rgb2grey_k(x_ptr, out_ptr, h, w, bs0: tl.constexpr, bs1: tl.constexpr):\n",
        "    \"\"\"\n",
        "    GPU kernel for converting RGB image to grayscale\n",
        "\n",
        "    Args:\n",
        "        x_ptr: Pointer to input RGB image data\n",
        "        out_ptr: Pointer to output grayscale image data\n",
        "        h: Image height\n",
        "        w: Image width\n",
        "        bs0: Block size for height dimension\n",
        "        bs1: Block size for width dimension\n",
        "    \"\"\"\n",
        "    # Get program IDs for parallel processing\n",
        "    pid_0 = tl.program_id(0)  # Block ID in height dimension\n",
        "    pid_1 = tl.program_id(1)  # Block ID in width dimension\n",
        "\n",
        "    # Calculate offsets for this block\n",
        "    offs_0 = pid_0 * bs0 + tl.arange(0, bs0)  # Offsets in height dimension\n",
        "    offs_1 = pid_1 * bs1 + tl.arange(0, bs1)  # Offsets in width dimension\n",
        "\n",
        "    # Calculate 2D offset matrix\n",
        "    offs = w * offs_0[:,None] + offs_1[None, :]\n",
        "\n",
        "    # Create masks to handle image boundaries\n",
        "    mask_0 = offs_0 < h\n",
        "    mask_1 = offs_1 < w\n",
        "    mask = mask_0[:,None] & mask_1[None,:]\n",
        "\n",
        "    # Load RGB channels\n",
        "    r = tl.load(x_ptr + 0*h*w + offs, mask=mask)\n",
        "    g = tl.load(x_ptr + 1*h*w + offs, mask=mask)\n",
        "    b = tl.load(x_ptr + 2*h*w + offs, mask=mask)\n",
        "\n",
        "    # Convert to grayscale using standard weights\n",
        "    # These weights represent human perception of color:\n",
        "    # Red: 29.89%, Green: 58.70%, Blue: 11.40%\n",
        "    out = 0.2989*r + 0.5870*g + 0.1140*b\n",
        "\n",
        "    # Store the result\n",
        "    tl.store(out_ptr + offs, out, mask=mask)\n",
        "\n",
        "def rgb2grey(x, bs):\n",
        "    \"\"\"\n",
        "    Convert RGB image to grayscale using GPU acceleration\n",
        "\n",
        "    Args:\n",
        "        x: Input RGB image tensor (channels, height, width)\n",
        "        bs: Tuple of block sizes (height, width) for GPU processing\n",
        "\n",
        "    Returns:\n",
        "        Grayscale image tensor (height, width)\n",
        "    \"\"\"\n",
        "    c, h, w = x.shape\n",
        "    # Create output tensor\n",
        "    out = torch.empty((h,w), dtype=x.dtype, device=x.device)\n",
        "\n",
        "    # Define processing grid based on block sizes\n",
        "    grid = lambda meta: (triton.cdiv(h, meta['bs0']), triton.cdiv(w, meta['bs1']))\n",
        "\n",
        "    # Launch GPU kernel\n",
        "    rgb2grey_k[grid](x, out, h, w, bs0=bs[0], bs1=bs[1])\n",
        "    return out.view(h,w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177cd277-b42b-4bd9-bcf7-eb5e9419a2ad",
      "metadata": {
        "id": "177cd277-b42b-4bd9-bcf7-eb5e9419a2ad"
      },
      "outputs": [],
      "source": [
        "# Convert image to grayscale and display\n",
        "grey_img = rgb2grey(img.to('cuda'), bs=(32, 32)).to('cpu')\n",
        "show_img(grey_img, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c329e5bf-260b-4073-87b7-6fe59922662e",
      "metadata": {
        "id": "c329e5bf-260b-4073-87b7-6fe59922662e"
      },
      "source": [
        "## Background\n",
        "\n",
        "We have a working Triton kernel `rgb2grey_k` that converts an RGB image to grayscale using manual pointer arithmetic:\n",
        "\n",
        "```python\n",
        "@triton.jit\n",
        "def rgb2grey_k(x_ptr, out_ptr, h, w, bs0: tl.constexpr, bs1: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    pid_1 = tl.program_id(1)\n",
        "\n",
        "    offs_0 = pid_0 * bs0 + tl.arange(0, bs0)\n",
        "    offs_1 = pid_1 * bs1 + tl.arange(0, bs1)\n",
        "\n",
        "    offs = w * offs_0[:, None] + offs_1[None, :]\n",
        "\n",
        "    mask_0 = offs_0 < h\n",
        "    mask_1 = offs_1 < w\n",
        "    mask = mask_0[:, None] & mask_1[None, :]\n",
        "\n",
        "    r = tl.load(x_ptr + 0 * h * w + offs, mask=mask)\n",
        "    g = tl.load(x_ptr + 1 * h * w + offs, mask=mask)\n",
        "    b = tl.load(x_ptr + 2 * h * w + offs, mask=mask)\n",
        "\n",
        "    out = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "\n",
        "    tl.store(out_ptr + offs, out, mask=mask)\n",
        "```\n",
        "\n",
        "Your goal is to rewrite this kernel using `tl.make_block_ptr`.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Understanding the Input Tensor Layout\n",
        "\n",
        "**Question 1.1:** The input image tensor `x` has shape `(3, h, w)` representing (channels, height, width). In PyTorch, what do `x.stride(0)`, `x.stride(1)`, and `x.stride(2)` represent? What are their values for a contiguous tensor?\n",
        "\n",
        "**Question 1.2:** In the original kernel, we access the red channel with `x_ptr + 0 * h * w + offs`. Explain why `h * w` is the offset between channels.\n",
        "\n",
        "**Question 1.3:** Why is it better to pass strides as kernel arguments rather than computing them as `h * w`?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61891430-7aa3-41b0-b53c-31df72e86bd8",
      "metadata": {
        "id": "61891430-7aa3-41b0-b53c-31df72e86bd8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1214ac57-cbbf-4bd4-8d5a-5c8993613668",
      "metadata": {
        "id": "1214ac57-cbbf-4bd4-8d5a-5c8993613668"
      },
      "outputs": [],
      "source": [
        "grey_img_t = rgb2grey(img.transpose(2,1).to('cuda'), bs=(32, 32)).to('cpu')\n",
        "show_img(grey_img_t.T, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c619824b-6af6-4174-bff7-31d7488e3712",
      "metadata": {
        "id": "c619824b-6af6-4174-bff7-31d7488e3712"
      },
      "source": [
        "<details>\n",
        "<summary><b>Claude's answer</b></summary>\n",
        "\n",
        "**Answer 1.1:**\n",
        "\n",
        "For a tensor `x` of shape `(3, h, w)`:\n",
        "- `x.stride(0)` = number of elements to skip to move to the next channel = `h * w`\n",
        "- `x.stride(1)` = number of elements to skip to move to the next row = `w`\n",
        "- `x.stride(2)` = number of elements to skip to move to the next column = `1`\n",
        "\n",
        "For a contiguous tensor with shape `(3, h, w)`:\n",
        "- `stride(0) = h * w`\n",
        "- `stride(1) = w`\n",
        "- `stride(2) = 1`\n",
        "\n",
        "**Answer 1.2:**\n",
        "\n",
        "In a contiguous tensor with shape `(3, h, w)`, the data is laid out in memory as:\n",
        "```\n",
        "[R_00, R_01, ..., R_0w, R_10, ..., R_hw, G_00, G_01, ..., G_hw, B_00, ...]\n",
        "```\n",
        "\n",
        "Each channel contains `h * w` elements. So to jump from the red channel (index 0) to the green channel (index 1), we need to skip `h * w` elements. This is exactly `stride(0)`.\n",
        "\n",
        "**Answer 1.3:**\n",
        "\n",
        "Passing strides as kernel arguments is better because:\n",
        "1. **Non-contiguous tensors**: If the tensor is a view, slice, or transposed, the strides may not equal `h * w`. Using explicit strides handles all cases correctly.\n",
        "2. **Flexibility**: The same kernel can work with different memory layouts.\n",
        "3. **Correctness**: Avoids bugs when tensors are not contiguous.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 2: Introduction to Block Pointers\n",
        "\n",
        "**Question 2.1:** What are the main advantages of using `tl.make_block_ptr` over manual pointer arithmetic?\n",
        "\n",
        "**Question 2.2:** The `tl.make_block_ptr` function has the following signature:\n",
        "\n",
        "```python\n",
        "tl.make_block_ptr(\n",
        "    base,          # Base pointer\n",
        "    shape,         # Shape of the underlying tensor\n",
        "    strides,       # Strides of the underlying tensor\n",
        "    offsets,       # Starting offsets for this block\n",
        "    block_shape,   # Shape of the block to load\n",
        "    order,         # Memory order\n",
        ")\n",
        "```\n",
        "\n",
        "For a 2D block of the red channel with shape `(bs0, bs1)`, starting at position `(offs_0, offs_1)`, fill in the parameters:\n",
        "\n",
        "```python\n",
        "r_block = tl.make_block_ptr(\n",
        "    base=???,\n",
        "    shape=???,\n",
        "    strides=???,\n",
        "    offsets=???,\n",
        "    block_shape=???,\n",
        "    order=(1, 0),\n",
        ")\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary><b>Claude's answer</b></summary>\n",
        "    \n",
        "**Answer 2.1:**\n",
        "\n",
        "Advantages of `tl.make_block_ptr`:\n",
        "1. **Cleaner code**: No manual offset calculations with `[:, None]` broadcasting\n",
        "2. **Automatic boundary handling**: Use `boundary_check` instead of manual masks\n",
        "3. **Better readability**: The shape and strides are explicit in the API\n",
        "4. **Less error-prone**: Reduces the chance of off-by-one errors or incorrect broadcasting\n",
        "5. **Enables `tl.advance()`**: Can easily move the block pointer to the next tile\n",
        "\n",
        "**Answer 2.2:**\n",
        "\n",
        "```python\n",
        "r_block = tl.make_block_ptr(\n",
        "    base=x_ptr + 0 * stride_x0,      # Pointer to start of red channel\n",
        "    shape=(h, w),                     # Shape of the 2D channel\n",
        "    strides=(stride_x1, stride_x2),   # Row and column strides\n",
        "    offsets=(offs_0, offs_1),         # Starting position for this block\n",
        "    block_shape=(bs0, bs1),           # Size of the block to load\n",
        "    order=(1, 0),                     # Memory order (row-major)\n",
        ")\n",
        "```\n",
        "</details>\n",
        "\n",
        "## Part 3: Handling Multiple Channels\n",
        "\n",
        "**Question 3.1:** Why can't we use a single 3D block pointer with shape `(3, bs0, bs1)` and then use `tl.sum` to compute the weighted average?\n",
        "\n",
        "*Hint: Think about Triton's constraints on reduction operations.*\n",
        "\n",
        "**Question 3.2:** Write the block pointer definitions for all three channels (R, G, B). The base pointer for each channel should be offset by the appropriate stride.\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>Claude's answer</b></summary>\n",
        "    \n",
        "**Answer 3.1:**\n",
        "\n",
        "Triton's `tl.sum` (and other reduction operations) require the reduction axis to have a **power-of-two** number of elements (2, 4, 8, 16, ...).\n",
        "\n",
        "RGB images have 3 channels, and 3 is not a power of two. Therefore, `tl.sum` would fail with an error like:\n",
        "```\n",
        "\"Number of elements must be power-of-two\"\n",
        "```\n",
        "\n",
        "This is why we must load each channel separately and compute the weighted sum manually.\n",
        "\n",
        "**Answer 3.2:**\n",
        "\n",
        "```python\n",
        "# Red channel (channel index 0)\n",
        "r_block = tl.make_block_ptr(\n",
        "    x_ptr + 0 * stride_x0,\n",
        "    shape=(h, w),\n",
        "    strides=(stride_x1, stride_x2),\n",
        "    offsets=(offs_0, offs_1),\n",
        "    block_shape=(bs0, bs1),\n",
        "    order=(1, 0),\n",
        ")\n",
        "\n",
        "# Green channel (channel index 1)\n",
        "g_block = tl.make_block_ptr(\n",
        "    x_ptr + 1 * stride_x0,\n",
        "    shape=(h, w),\n",
        "    strides=(stride_x1, stride_x2),\n",
        "    offsets=(offs_0, offs_1),\n",
        "    block_shape=(bs0, bs1),\n",
        "    order=(1, 0),\n",
        ")\n",
        "\n",
        "# Blue channel (channel index 2)\n",
        "b_block = tl.make_block_ptr(\n",
        "    x_ptr + 2 * stride_x0,\n",
        "    shape=(h, w),\n",
        "    strides=(stride_x1, stride_x2),\n",
        "    offsets=(offs_0, offs_1),\n",
        "    block_shape=(bs0, bs1),\n",
        "    order=(1, 0),\n",
        ")\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 4: Loading and Storing with Block Pointers\n",
        "\n",
        "When using `tl.load` with a block pointer, we use `boundary_check` instead of `mask`. `boundary_check=(0, 1)` means:\n",
        "- Check boundaries along axis 0 (rows/height dimension)\n",
        "- Check boundaries along axis 1 (columns/width dimension)\n",
        "\n",
        "This ensures that when the block extends beyond the tensor boundaries (e.g., at the edges of the image), Triton handles it safely instead of causing out-of-bounds memory access.\n",
        "\n",
        "`padding_option=\"zero\"` specifies that when loading data outside the tensor boundaries, the out-of-bounds values should be filled with zeros.\n",
        "\n",
        "Other options include:\n",
        "- `padding_option=\"nan\"`: Fill with NaN values\n",
        "\n",
        "This is useful for handling edge cases where the image dimensions are not divisible by the block size.\n",
        "\n",
        "The original kernel outputs `uint8` values. When we compute `0.2989 * r + 0.5870 * g + 0.1140 * b`:\n",
        "- The constants `0.2989`, `0.5870`, `0.1140` are `float32`\n",
        "- The multiplication promotes `r`, `g`, `b` (originally `uint8`) to `float32`\n",
        "- The result `out` is `float32`\n",
        "\n",
        "To convert back to `uint8` before storing:\n",
        "```python\n",
        "tl.store(out_block, out.to(tl.uint8), boundary_check=(0, 1))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Part 5: Complete Implementation\n",
        "\n",
        "**Question 5.1:** Complete the following kernel using block pointers:\n",
        "\n",
        "```python\n",
        "@triton.jit\n",
        "def rgb2grey_kernel(\n",
        "    x_ptr, out_ptr,\n",
        "    stride_x0, stride_x1, stride_x2,\n",
        "    stride_out0, stride_out1,\n",
        "    h, w,\n",
        "    bs0: tl.constexpr, bs1: tl.constexpr,\n",
        "):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    pid_1 = tl.program_id(1)\n",
        "\n",
        "    offs_0 = pid_0 * bs0\n",
        "    offs_1 = pid_1 * bs1\n",
        "\n",
        "    # TODO: Create block pointers for R, G, B channels\n",
        "    r_block = ???\n",
        "    g_block = ???\n",
        "    b_block = ???\n",
        "    out_block = ???\n",
        "\n",
        "    # TODO: Load data from each channel\n",
        "    r = ???\n",
        "    g = ???\n",
        "    b = ???\n",
        "\n",
        "    # TODO: Compute grayscale and store\n",
        "    out = ???\n",
        "    tl.store(???)\n",
        "```\n",
        "\n",
        "**Question 5.2:** Write the wrapper function that:\n",
        "- Creates the output tensor with the correct dtype\n",
        "- Computes the grid dimensions\n",
        "- Launches the kernel with all required arguments\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "830dc859-37ca-4588-8745-ca5ff8959408",
      "metadata": {
        "id": "830dc859-37ca-4588-8745-ca5ff8959408"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def rgb2grey_kernel(\n",
        "    x_ptr,\n",
        "    out_ptr,\n",
        "    stride_x0,\n",
        "    stride_x1,\n",
        "    stride_x2,\n",
        "    stride_out0,\n",
        "    stride_out1,\n",
        "    h,\n",
        "    w,\n",
        "    bs0: tl.constexpr,\n",
        "    bs1: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    GPU kernel for converting RGB image to grayscale\n",
        "\n",
        "    Args:\n",
        "        x_ptr: Pointer to input RGB image data\n",
        "        out_ptr: Pointer to output grayscale image data\n",
        "        h: Image height\n",
        "        w: Image width\n",
        "        bs0: Block size for height dimension\n",
        "        bs1: Block size for width dimension\n",
        "    \"\"\"\n",
        "    # Get program IDs for parallel processing\n",
        "    pid_0 = tl.program_id(0)  # Block ID in height dimension\n",
        "    pid_1 = tl.program_id(1)  # Block ID in width dimension\n",
        "\n",
        "    # Calculate offsets for this block\n",
        "    offs_0 = pid_0 * bs0    # Offsets in height dimension\n",
        "    offs_1 = pid_1 * bs1    # Offsets in width dimension\n",
        "    # Separate 2D block pointer for each channel\n",
        "    r_block = tl.make_block_ptr(\n",
        "        # your code here\n",
        "    )\n",
        "    g_block = tl.make_block_ptr(\n",
        "        # your code here\n",
        "    )\n",
        "    b_block = tl.make_block_ptr(\n",
        "        # your code here\n",
        "    )\n",
        "    out_block = tl.make_block_ptr(\n",
        "        # your code here\n",
        "    )\n",
        "\n",
        "    # Load each channel separately\n",
        "    r = # your code here\n",
        "    g = # your code here\n",
        "    b = # your code here\n",
        "\n",
        "    # Compute grayscale\n",
        "    out = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "\n",
        "    tl.store(# your code here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f2b545-10f3-4a42-a761-ce4ec65a4ae1",
      "metadata": {
        "id": "18f2b545-10f3-4a42-a761-ce4ec65a4ae1"
      },
      "outputs": [],
      "source": [
        "def rgb2grey_block(x, bs):\n",
        "    \"\"\"\n",
        "    Convert RGB image to grayscale using GPU acceleration\n",
        "\n",
        "    Args:\n",
        "        x: Input RGB image tensor (channels, height, width)\n",
        "        bs: Tuple of block sizes (height, width) for GPU processing\n",
        "\n",
        "    Returns:\n",
        "        Grayscale image tensor (height, width)\n",
        "    \"\"\"\n",
        "    c, h, w = x.shape\n",
        "    # Create output tensor\n",
        "    out = # your code here\n",
        "\n",
        "    # Define processing grid based on block sizes\n",
        "    grid = # your code here\n",
        "\n",
        "    # Launch GPU kernel\n",
        "    rgb2grey_kernel[grid](\n",
        "        x,\n",
        "        out,\n",
        "        x.stride(0),\n",
        "        x.stride(1),\n",
        "        x.stride(2),\n",
        "        out.stride(0),\n",
        "        out.stride(1),\n",
        "        h,\n",
        "        w,\n",
        "        bs0=bs[0],\n",
        "        bs1=bs[1],\n",
        "    )\n",
        "    return out.view(h, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d747f4-e04b-4a71-a4c7-8835d6f183ea",
      "metadata": {
        "id": "45d747f4-e04b-4a71-a4c7-8835d6f183ea"
      },
      "outputs": [],
      "source": [
        "grey_img_block = rgb2grey_block(img.to('cuda'), bs=(32, 32)).to('cpu')\n",
        "show_img(grey_img_block, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "942647a1-8954-4552-b886-da967bd84213",
      "metadata": {
        "id": "942647a1-8954-4552-b886-da967bd84213"
      },
      "outputs": [],
      "source": [
        "assert torch.equal(grey_img, grey_img_block)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ef9dc1-9945-44df-beeb-8657d8195501",
      "metadata": {
        "id": "28ef9dc1-9945-44df-beeb-8657d8195501"
      },
      "source": [
        "## Part 6: Messing up with transpositions\n",
        "\n",
        "Now your code should work fine even with non-contiguous tensors!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2299ec2-7d39-4413-88f5-22bbc11350c8",
      "metadata": {
        "id": "b2299ec2-7d39-4413-88f5-22bbc11350c8"
      },
      "outputs": [],
      "source": [
        "grey_img_block_t = rgb2grey_block(img.transpose(2,1).to('cuda'), bs=(32, 32)).to('cpu')\n",
        "show_img(grey_img_block_t.T, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5202daf2-0ed0-4621-9233-598a106136b8",
      "metadata": {
        "id": "5202daf2-0ed0-4621-9233-598a106136b8"
      },
      "source": [
        "# Fused Softmax-Matmul\n",
        "\n",
        "Now you will have to code the triton kernel for the following function for `x` (shape `batch_size, d1, d2`) and `V` (shape `batch_size, d2, d3`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7fd3e55-ce32-40b6-9415-824636c55754",
      "metadata": {
        "id": "d7fd3e55-ce32-40b6-9415-824636c55754"
      },
      "outputs": [],
      "source": [
        "def softmax_mult(x, V, dim=-1):\n",
        "    return F.softmax(x, dim=dim) @ V"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1377566-6041-46b5-96b3-50fd53daca3c",
      "metadata": {
        "id": "f1377566-6041-46b5-96b3-50fd53daca3c"
      },
      "source": [
        "## Operator Fusion: Fused Softmax-Matmul Kernel\n",
        "\n",
        "To be efficient, your kernel should compute `softmax(x) @ V` in a **single fused operation** rather than two separate steps.\n",
        "\n",
        "For more details, we recommend [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html) by [Horace He](https://horace.io/index.html) (illustrations below are taken from this blog).\n",
        "\n",
        "<table><tr>\n",
        "<td> <img src=\"https://horace.io/img/perf_intro/multi_operators.png\" width=\"500\"/> </td>\n",
        "<td>   VS </td>\n",
        "<td> <img src=\"https://horace.io/img/perf_intro/operator_fusion.png\" width=\"500\"/> </td>\n",
        "</tr></table>\n",
        "\n",
        "### The Problem: Naive Two-Kernel Approach\n",
        "\n",
        "**Unfused version (BAD for performance):**\n",
        "```python\n",
        "# Kernel 1: Compute and store full softmax\n",
        "softmax_x = softmax(x)  # Shape: (batch, d1, d2)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Kernel 2: Matrix multiply\n",
        "output = softmax_x @ V  # Shape: (batch, d1, d3)\n",
        "```\n",
        "\n",
        "**Why this is inefficient:**\n",
        "1. **Memory bottleneck**: Must write `softmax_x` to GPU memory (size: `batch × d1 × d2`)\n",
        "2. **Memory bottleneck again**: Must read `softmax_x` back from memory\n",
        "3. **Example**: For attention with `d1 = d2 = 4096`, softmax output is 67MB per batch element!\n",
        "\n",
        "\n",
        "## Matrix Multiplication as Sum of Outer Products\n",
        "\n",
        "Given a pair of matrices **A** of size m×p and **B** of size p×n, consider the matrix product **C** = **AB** defined as usual as a matrix of size m×n.\n",
        "Then **C** can be expressed as a sum of column-by-row outer products:\n",
        "\n",
        "$$\\mathbf{C} = \\mathbf{AB} = \\left(\\sum_{k=1}^{p} A_{ik}B_{kj}\\right)_{\\substack{1 \\leq i \\leq m \\\\ 1 \\leq j \\leq n}} = \\begin{bmatrix} \\mathbf{a}_1^{\\text{col}} & \\cdots & \\mathbf{a}_p^{\\text{col}} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b}_1^{\\text{row}} \\\\ \\vdots \\\\ \\mathbf{b}_p^{\\text{row}} \\end{bmatrix} = \\sum_{k=1}^{p} \\mathbf{a}_k^{\\text{col}} \\otimes \\mathbf{b}_k^{\\text{row}}$$\n",
        "\n",
        "In our setting, if `Y = softmax(x) @ V`, we have $Y = \\sum_{i=1}^{d_2} s_i \\otimes v[i,:]$ where $s_i\\in \\mathbb{R}^d_1$ is the column of the softmax matrix. Hence, we can can adpat our noline softamx algorithm to compute $Y$ as follows:\n",
        "\n",
        "## Algorithm: not-flash-attention\n",
        "\n",
        "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R^{d_1}}$ and $v \\in \\mathbb{R^{d_2\\times d_3}}$\n",
        "\n",
        "**Output:** $o_1, \\ldots, o_{d_2}\\in \\mathbb{R^{d_3}}$\n",
        "\n",
        "**First pass:** for $i = 1, \\ldots, d_2$\n",
        "$$\n",
        "\\begin{align}\n",
        "m_0 &= -\\infty \\\\\n",
        "\\ell'_0 &= 0 \\\\\n",
        "& m_i \\leftarrow \\max(m_{i-1}, x_i) \\\\\n",
        "& \\ell'_i \\leftarrow \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i - m_i}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Second pass:** for $i = 1, \\ldots, d_2$\n",
        "$$\n",
        "\\begin{align}\n",
        "o_0 &= 0 \\\\\n",
        "&o_i \\leftarrow o_{i-1}  + \\frac{e^{x_i - m_{d_2}}}{\\ell'_{d_2}}\\otimes v[i,:] \\in \\mathbb{R}^{d_1\\times d_3}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "With this algorithm, we have $o_{d_2} = Y = \\text{softmax}(x) @ V$\n",
        "\n",
        "In order to get the same result in only one pass, let apply the same trick as before and consider:\n",
        "$$\n",
        "\\begin{align}\n",
        "o'_i &= \\sum_{j\\leq  i} \\frac{e^{x_j - m_{i}}}{\\ell'_{i}}\\otimes v[j,:]\\\\\n",
        "&= e^{m_{i-1}-m_i}\\frac{\\ell'_{i-1}}{\\ell'_i} \\odot \\left( \\sum_{j\\leq  i-1} \\frac{e^{x_j - m_{i-1}}}{\\ell'_{i-1}}\\otimes v[j,:]\\right) + \\frac{e^{x_i - m_{i}}}{\\ell'_{i}}\\otimes v[i,:]\\\\\n",
        "&= e^{m_{i-1}-m_i}\\frac{\\ell'_{i-1}}{\\ell'_i} \\odot o'_{i-1} + \\frac{e^{x_i - m_{i}}}{\\ell'_{i}}\\otimes v[i,:]\n",
        "\\end{align}\n",
        "$$\n",
        "where $\\odot$ is the notation for broadcasting (i.e. here multiplying a vector in $\\mathbb{R}^{d_1}$ and a matrix in $\\mathbb{R}^{d_1\\times d_3}$).\n",
        "Clearly, we have $o'_{d_2} = o_{d_2}$, so that we are ready for our one-pass algorithm\n",
        "\n",
        "## Algorithm: almost-flash-attention\n",
        "\n",
        "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R^{d_1}}$ and $v \\in \\mathbb{R^{d_2\\times d_3}}$\n",
        "\n",
        "**Output:** $o_1, \\ldots, o_{d_2}\\in \\mathbb{R^{d_3}}$\n",
        "\n",
        "**One pass:** for $i = 1, \\ldots, d_2$\n",
        "$$\n",
        "\\begin{align}\n",
        "m_0 &= -\\infty \\\\\n",
        "\\ell'_0 &= 0 \\\\\n",
        "o'_0 &=0\\\\\n",
        "& m_i \\leftarrow \\max(m_{i-1}, x_i) \\\\\n",
        "& \\ell'_i \\leftarrow \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i - m_i}\\\\\n",
        "&o'_i \\leftarrow e^{m_{i-1}-m_i}\\frac{\\ell'_{i-1}}{\\ell'_i} \\odot o'_{i-1} + \\frac{e^{x_i - m_i}}{\\ell'_i}\\otimes v[i,:]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "### Operator Fusion: Do Both Operations in One Kernel\n",
        "\n",
        "**Key insight:** We can compute the final result **without ever materializing the full softmax matrix**.\n",
        "\n",
        "\n",
        "### Memory Savings\n",
        "\n",
        "**Unfused:**\n",
        "- Read `x`: `batch × d1 × d2`\n",
        "- Write `softmax(x)`: `batch × d1 × d2` ❌\n",
        "- Read `softmax(x)`: `batch × d1 × d2` ❌\n",
        "- Read `V`: `batch × d2 × d3`\n",
        "- Write `output`: `batch × d1 × d3`\n",
        "- **Total:** `3 × d1 × d2 + d2 × d3 + d1 × d3`\n",
        "\n",
        "**Fused:**\n",
        "- Read `x`: `batch × d1 × d2`\n",
        "- Read `V`: `batch × d2 × d3`\n",
        "- Write `output`: `batch × d1 × d3`\n",
        "- **Total:** `d1 × d2 + d2 × d3 + d1 × d3`\n",
        "\n",
        "**Savings:** Eliminated `2 × d1 × d2` memory accesses!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e23842f-6ea0-4b05-ab6a-ad8e4cf3f0d0",
      "metadata": {
        "id": "4e23842f-6ea0-4b05-ab6a-ad8e4cf3f0d0"
      },
      "source": [
        "## Exercise: Fused Softmax-MatMul Kernel (Flash Attention-style)\n",
        "\n",
        "### Objective\n",
        "Implement a Triton kernel that computes `softmax(X) @ V` in a fused manner, where:\n",
        "- `X` has shape `(batch_size, d1, d2)`\n",
        "- `V` has shape `(batch_size, d2, d3)`\n",
        "- Output has shape `(batch_size, d1, d3)`\n",
        "\n",
        "This kernel combines the online softmax algorithm with matrix multiplication, avoiding the need to materialize the full softmax output in memory.\n",
        "\n",
        "\n",
        "### The Algorithm: Almost-Flash-Attention\n",
        "\n",
        "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R}^{d_1}$ and $v \\in \\mathbb{R}^{d_2 \\times d_3}$\n",
        "\n",
        "**Output:** $o_1, \\ldots, o_{d_2} \\in \\mathbb{R}^{d_3}$\n",
        "\n",
        "**One pass:** for $i = 1, \\ldots, d_2$\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "m_0 &= -\\infty \\\\\n",
        "\\ell'_0 &= 0 \\\\\n",
        "o'_0 &= 0\\\\\n",
        "m_i &\\leftarrow \\max(m_{i-1}, x_i) \\\\\n",
        "\\ell'_i &\\leftarrow \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i - m_i}\\\\\n",
        "o'_i &\\leftarrow e^{m_{i-1}-m_i}\\frac{\\ell'_{i-1}}{\\ell'_i} \\odot o'_{i-1} + \\frac{e^{x_i - m_i}}{\\ell'_i} \\otimes v[i,:]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "1. **Online Softmax:** Process `X` in blocks, maintaining running max (`m`) and sum (`l`)\n",
        "2. **Fusion:** Immediately multiply softmax results with `V` blocks—never store full softmax\n",
        "3. **Rescaling:** When `m` updates, rescale previous output contributions to maintain consistency\n",
        "\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Complete the missing parts in the Triton kernel. The code for the wrapper function is given below.\n",
        "\n",
        "### Hints\n",
        "\n",
        "1. **Block pointers:** Use `tl.make_block_ptr()` with appropriate base pointer, shape, strides, block_shape, and order\n",
        "2. **Advancing pointers:** Use tuple `(0, BLOCK_2)` to advance along d2 dimension\n",
        "3. **Online softmax:** Remember to update both `m_curr` and `l_curr` before computing output\n",
        "4. **Rescaling:** The scale factor is `(l_prev / l_curr) * exp(m_prev - m_curr)`\n",
        "5. **Broadcasting:** Use `[:, None]` to broadcast vectors for element-wise operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3126a204-d5d4-486f-a1a9-1ddc8f0c463c",
      "metadata": {
        "id": "3126a204-d5d4-486f-a1a9-1ddc8f0c463c"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def fused_softmax_triton_kernel(\n",
        "    x_ptr,\n",
        "    V_ptr,\n",
        "    output_ptr,\n",
        "    stride_xbatch,\n",
        "    stride_xrow,\n",
        "    stride_xcol,\n",
        "    stride_Vbatch,\n",
        "    stride_Vrow,\n",
        "    stride_Vcol,\n",
        "    stride_outbatch,\n",
        "    stride_outrow,\n",
        "    stride_outcol,\n",
        "    d1: tl.constexpr,\n",
        "    d2: tl.constexpr,\n",
        "    d3: tl.constexpr,\n",
        "    BLOCK_1: tl.constexpr,\n",
        "    BLOCK_2: tl.constexpr,\n",
        "):\n",
        "    tl.static_assert(d2 % BLOCK_2 == 0, \"d2 must be divisible by BLOCK_2\")\n",
        "    tl.static_assert(d1 % BLOCK_1 == 0, \"d1 must be divisible by BLOCK_1\")\n",
        "\n",
        "    pid_batch = tl.program_id(0)\n",
        "    pid_row = tl.program_id(1)\n",
        "\n",
        "    x_block = tl.make_block_ptr(\n",
        "        # your code here\n",
        "    )\n",
        "\n",
        "    V_block = tl.make_block_ptr(\n",
        "        # your code here\n",
        "    )\n",
        "\n",
        "    output_block = tl.make_block_ptr(\n",
        "        # your code here\n",
        "    )\n",
        "\n",
        "    Num_blocks = tl.cdiv(d2, BLOCK_2)\n",
        "\n",
        "    m_prev = tl.full((BLOCK_1,), float(\"-inf\"), dtype=tl.float32)\n",
        "    l_prev = tl.zeros((BLOCK_1,), dtype=tl.float32)\n",
        "    out_prev = tl.zeros((BLOCK_1, d3), dtype=tl.float32)\n",
        "\n",
        "    for _ in range(Num_blocks):\n",
        "        x = tl.load(# your code here)\n",
        "        v = tl.load(# your code here)\n",
        "\n",
        "        # Compute block max (Hint: use tl.max and tl.maximum)\n",
        "        # your code here\n",
        "\n",
        "        # Update running sum with rescaling (Hint: use tl.exp and tl.sum)\n",
        "        # your code here\n",
        "\n",
        "        # Scale and accumulate\n",
        "        # depending on hardware:\n",
        "        # on Turing (T4, RTX8000), cast to tl.float16 for tl.dot product only because of this bug: https://github.com/triton-lang/triton/issues/5557\n",
        "        # on Hopper (H100), no casting necessary\n",
        "        # your code here\n",
        "\n",
        "        x_block = x_block.advance(# your code here)\n",
        "        V_block = V_block.advance(# your code here)\n",
        "\n",
        "    tl.store(# your code here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9244928a-5fb9-441b-b63d-311ff0f772cb",
      "metadata": {
        "id": "9244928a-5fb9-441b-b63d-311ff0f772cb"
      },
      "outputs": [],
      "source": [
        "def fused_softmax_triton(x, V, BLOCK_1=16, BLOCK_2=16):\n",
        "\n",
        "    batch_size, d1, d2 = x.shape\n",
        "    bs, d2 ,d3 = V.shape\n",
        "    assert batch_size == bs, \"Batch size of x and V must match\"\n",
        "    assert d2 == d2, \"d2 of x and V must match\"\n",
        "    fused_softmax_output = torch.empty((batch_size, d1, d3), device=x.device, dtype=x.dtype)\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    grid = (batch_size, triton.cdiv(d1, BLOCK_1))\n",
        "\n",
        "    # Launch kernel\n",
        "    fused_softmax_triton_kernel[grid](\n",
        "        x,\n",
        "        V,\n",
        "        fused_softmax_output,\n",
        "        x.stride(0),\n",
        "        x.stride(1),\n",
        "        x.stride(2),\n",
        "        V.stride(0),\n",
        "        V.stride(1),\n",
        "        V.stride(2),\n",
        "        fused_softmax_output.stride(0),\n",
        "        fused_softmax_output.stride(1),\n",
        "        fused_softmax_output.stride(2),\n",
        "        d1,\n",
        "        d2,\n",
        "        d3,\n",
        "        BLOCK_1=BLOCK_1,\n",
        "        BLOCK_2=BLOCK_2,\n",
        "    )\n",
        "\n",
        "    return fused_softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca713cb-79d5-4759-859c-da6d627d34ba",
      "metadata": {
        "id": "4ca713cb-79d5-4759-859c-da6d627d34ba"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "dtype = torch.float32\n",
        "batch_size = 8\n",
        "d1 = 2048\n",
        "d2 = 8192\n",
        "d3 = 64\n",
        "B = 16\n",
        "x = torch.randn(batch_size, d1, d2, device=device, dtype=dtype)\n",
        "V = torch.randn(batch_size, d2, d3, device=device, dtype=dtype)\n",
        "times_triton = time_loop(lambda: fused_softmax_triton(x, V, BLOCK_1=B, BLOCK_2=B), 10)\n",
        "times_pytorch = time_loop(lambda: softmax_mult(x,V), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e796cd4e-d140-4119-bd64-1d98a0b4e4ef",
      "metadata": {
        "id": "e796cd4e-d140-4119-bd64-1d98a0b4e4ef"
      },
      "outputs": [],
      "source": [
        "print(f\"TRITON bench ms_mean: {np.mean(times_triton)} and ms_std: {np.std(times_triton)}\")\n",
        "print(f\"PYTORCH bench ms_mean: {np.mean(times_pytorch)} and ms_std: {np.std(times_pytorch)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe80e9d3-c004-48db-927e-3960cd4f5f94",
      "metadata": {
        "id": "fe80e9d3-c004-48db-927e-3960cd4f5f94"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "dtype = torch.float32\n",
        "batch_size = 8\n",
        "d1 = 8192\n",
        "d2 = 8192\n",
        "d3 = 64\n",
        "B = 16\n",
        "x = torch.randn(batch_size, d1, d2, device=device, dtype=dtype)\n",
        "V = torch.randn(batch_size, d2, d3, device=device, dtype=dtype)\n",
        "times_triton = time_loop(lambda: fused_softmax_triton(x, V, BLOCK_1=B, BLOCK_2=B), 10)\n",
        "times_pytorch = time_loop(lambda: softmax_mult(x,V), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da53b06-d369-4d21-89e6-7d124f186809",
      "metadata": {
        "id": "9da53b06-d369-4d21-89e6-7d124f186809"
      },
      "outputs": [],
      "source": [
        "print(f\"TRITON bench ms_mean: {np.mean(times_triton)} and ms_std: {np.std(times_triton)}\")\n",
        "print(f\"PYTORCH bench ms_mean: {np.mean(times_pytorch)} and ms_std: {np.std(times_pytorch)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66790705-bcb6-48dd-872e-88311af0b04d",
      "metadata": {
        "id": "66790705-bcb6-48dd-872e-88311af0b04d"
      },
      "source": [
        "# Memory profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df69477-ec66-47f8-8ec4-e4c145866d25",
      "metadata": {
        "id": "9df69477-ec66-47f8-8ec4-e4c145866d25"
      },
      "outputs": [],
      "source": [
        "# Create test inputs\n",
        "batch_size, d1, d2, d3 = 2, 2048, 8192, 512\n",
        "device = \"cuda\"\n",
        "dtype = torch.float32\n",
        "x = torch.randn(batch_size, d1, d2, device=device, dtype=dtype)\n",
        "V = torch.randn(batch_size, d2, d3, device=device, dtype=dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4f168f2-5a42-4f63-b004-588965d6d706",
      "metadata": {
        "id": "f4f168f2-5a42-4f63-b004-588965d6d706"
      },
      "outputs": [],
      "source": [
        "def measure_peak_memory(fn, *args, **kwargs):\n",
        "    \"\"\"Measure peak GPU memory allocated during function execution.\"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    fn(*args, **kwargs)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert to MB\n",
        "    return peak_memory\n",
        "\n",
        "def comprehensive_memory_report(x, V):\n",
        "    \"\"\"Report both absolute and relative memory usage.\"\"\"\n",
        "\n",
        "    # Inputs size\n",
        "    input_size = (x.element_size() * x.numel() + V.element_size() * V.numel()) / (1024 ** 2)\n",
        "\n",
        "    # Method 1: Absolute peak\n",
        "    pytorch_peak = measure_peak_memory(lambda: F.softmax(x, dim=-1) @ V)\n",
        "    triton_peak = measure_peak_memory(lambda: fused_softmax_triton(x, V))\n",
        "\n",
        "    # Method 2: Additional allocations (run detailed comparison)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    start = torch.cuda.memory_allocated()\n",
        "    result = F.softmax(x, dim=-1) @ V\n",
        "    pytorch_extra = (torch.cuda.max_memory_allocated() - start) / (1024 ** 2)\n",
        "    del result\n",
        "\n",
        "    torch.cuda.empty_cache()              # Releases CACHED memory, but NOT active tensors\n",
        "    torch.cuda.reset_peak_memory_stats()  # Resets peak counter, doesn't free memory\n",
        "    start = torch.cuda.memory_allocated() # Returns memory of ACTIVE tensors (x and V)\n",
        "    result = fused_softmax_triton(x, V)\n",
        "    triton_extra = (torch.cuda.max_memory_allocated() - start) / (1024 ** 2)\n",
        "    del result\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"MEMORY USAGE COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nInput size: {input_size:.2f} MB\")\n",
        "    print(f\"  x: {x.shape} = {x.element_size() * x.numel() / (1024**2):.2f} MB\")\n",
        "    print(f\"  V: {V.shape} = {V.element_size() * V.numel() / (1024**2):.2f} MB\")\n",
        "\n",
        "    print(f\"\\n--- Peak Memory (Total GPU Usage) ---\")\n",
        "    print(f\"PyTorch unfused: {pytorch_peak:.2f} MB\")\n",
        "    print(f\"Triton fused:    {triton_peak:.2f} MB\")\n",
        "    print(f\"Absolute savings: {pytorch_peak - triton_peak:.2f} MB ({100 * (pytorch_peak - triton_peak) / pytorch_peak:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n--- Additional Allocations (Beyond Inputs) ---\")\n",
        "    print(f\"PyTorch unfused: {pytorch_extra:.2f} MB\")\n",
        "    print(f\"  └─ softmax intermediate: ~{x.shape[0] * x.shape[1] * x.shape[2] * x.element_size() / (1024**2):.2f} MB\")\n",
        "    print(f\"  └─ output: ~{x.shape[0] * x.shape[1] * V.shape[2] * x.element_size() / (1024**2):.2f} MB\")\n",
        "    print(f\"Triton fused:    {triton_extra:.2f} MB\")\n",
        "    print(f\"  └─ output only: ~{x.shape[0] * x.shape[1] * V.shape[2] * x.element_size() / (1024**2):.2f} MB\")\n",
        "    print(f\"Fusion benefit: {pytorch_extra - triton_extra:.2f} MB ({100 * (pytorch_extra - triton_extra) / pytorch_extra:.1f}%)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"KEY INSIGHT:\")\n",
        "    print(f\"Fusion eliminates the {x.shape[0] * x.shape[1] * x.shape[2] * x.element_size() / (1024**2):.2f} MB softmax intermediate,\")\n",
        "    print(f\"saving {100 * (pytorch_peak - triton_peak) / pytorch_peak:.1f}% of absolute memory allocations.\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "comprehensive_memory_report(x, V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b679b6b-f087-413e-82ec-dd5236160b67",
      "metadata": {
        "id": "1b679b6b-f087-413e-82ec-dd5236160b67"
      },
      "outputs": [],
      "source": [
        "batch_size, d1, d2, d3 = 8, 16384, 16384, 512\n",
        "device = \"cuda\"\n",
        "dtype = torch.float32\n",
        "x = torch.randn(batch_size, d1, d2, device=device, dtype=dtype)\n",
        "V = torch.randn(batch_size, d2, d3, device=device, dtype=dtype)\n",
        "comprehensive_memory_report(x, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7564f32f-1c12-4e73-bb7a-68f6420294a6",
      "metadata": {
        "id": "7564f32f-1c12-4e73-bb7a-68f6420294a6"
      },
      "source": [
        "### Mathematical Explanation for square attention matrices (d₁=d₂=n)\n",
        "```\n",
        "Input memory:    O(n²) + O(n·d₃)  [x and V]\n",
        "Softmax memory:  O(n²)            [intermediate]\n",
        "Output memory:   O(n·d₃)          [result]\n",
        "\n",
        "For large n:\n",
        "  PyTorch peak ≈ n² + n² + n·d₃ ≈ 2n²     (when n >> d₃)\n",
        "  Triton peak  ≈ n² + n·d₃      ≈ n²\n",
        "  Savings %    ≈ n²/(2n²)        = 50%\n",
        "```\n",
        "\n",
        "So the **asymptotic savings is ~50%** for square attention matrices (d₁=d₂=n).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e124ee-82aa-45f1-8f52-732776429604",
      "metadata": {
        "id": "e7e124ee-82aa-45f1-8f52-732776429604"
      },
      "source": [
        "### Connection with \"Flash Attention\"\n",
        "\n",
        "This exact fusion pattern is the core of **FlashAttention-2** [(Dao, 2023)](https://arxiv.org/abs/2307.08691):\n",
        "\n",
        "![](https://github.com/dataflowr/gpu_llm_flash-attention/blob/main/images/flash-att.png?raw=1)\n",
        "\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Operator fusion** = combining multiple operations into one kernel\n",
        "2. **Benefits**: Reduced memory traffic (major GPU bottleneck), faster execution\n",
        "3. **Trade-off**: More complex kernel code, requires careful accumulation logic\n",
        "4. **Online algorithm** enables fusion by computing results incrementally without materializing intermediates\n",
        "\n",
        "\n",
        "![](https://github.com/dataflowr/gpu_llm_flash-attention/blob/main/images/first.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e207878-7239-48b7-bfad-fe51708aa8b3",
      "metadata": {
        "id": "6e207878-7239-48b7-bfad-fe51708aa8b3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "chess",
      "language": "python",
      "name": "chess"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}